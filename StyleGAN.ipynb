{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=16, size=(3, 64, 64), nrow=3):\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu().clamp_(0, 1)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow, padding=0)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "def get_truncated_noise(n_samples, z_dim, truncation):\n",
    "    truncated_noise = truncnorm.rvs(-1*truncation, truncation, size=(n_samples, z_dim))\n",
    "    return torch.Tensor(truncated_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tuple(get_truncated_noise(n_samples=10, z_dim=5, truncation=0.7).shape) == (10, 5)\n",
    "simple_noise = get_truncated_noise(n_samples=1000, z_dim=10, truncation=0.2)\n",
    "assert simple_noise.max() > 0.199 and simple_noise.max() < 2\n",
    "assert simple_noise.min() < -0.199 and simple_noise.min() > -0.2\n",
    "assert simple_noise.std() > 0.113 and simple_noise.std() < 0.117\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingLayers(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, w_dim):\n",
    "        super().__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            nn.Linear(z_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, w_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        return self.mapping(noise)\n",
    "\n",
    "    def get_mapping(self):\n",
    "        return self.mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_fn = MappingLayers(10,20,30)\n",
    "assert tuple(map_fn(torch.randn(2, 10)).shape) == (2, 30)\n",
    "assert len(map_fn.mapping) > 4\n",
    "outputs = map_fn(torch.randn(1000, 10))\n",
    "assert outputs.std() > 0.05 and outputs.std() < 0.3\n",
    "assert outputs.min() > -2 and outputs.min() < 0\n",
    "assert outputs.max() < 2 and outputs.max() > 0\n",
    "layers = [str(x) for x in map_fn.get_mapping()]\n",
    "assert layers == ['Linear(in_features=10, out_features=20, bias=True)', \n",
    "                  'ReLU()', \n",
    "                  'Linear(in_features=20, out_features=20, bias=True)', \n",
    "                  'ReLU()', \n",
    "                  'Linear(in_features=20, out_features=30, bias=True)']\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InjectNoise(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(channels)[None, :, None, None] \n",
    "        )\n",
    "\n",
    "    def forward(self, image):        \n",
    "        noise_shape = (image.shape[0],1,image.shape[2],image.shape[3])\n",
    "        \n",
    "        noise = torch.randn(noise_shape, device=image.device) \n",
    "        return image + self.weight * noise nel\n",
    "    \n",
    "    def get_weight(self):\n",
    "        return self.weight\n",
    "    \n",
    "    def get_self(self):\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noise_channels = 3000\n",
    "test_noise_samples = 20\n",
    "fake_images = torch.randn(test_noise_samples, test_noise_channels, 10, 10)\n",
    "inject_noise = InjectNoise(test_noise_channels)\n",
    "assert torch.abs(inject_noise.weight.std() - 1) < 0.1\n",
    "assert torch.abs(inject_noise.weight.mean()) < 0.1\n",
    "assert type(inject_noise.get_weight()) == torch.nn.parameter.Parameter\n",
    "\n",
    "assert tuple(inject_noise.weight.shape) == (1, test_noise_channels, 1, 1)\n",
    "inject_noise.weight = nn.Parameter(torch.ones_like(inject_noise.weight))\n",
    "\n",
    "assert torch.abs((inject_noise(fake_images) - fake_images)).mean() > 0.1\n",
    "\n",
    "assert torch.abs((inject_noise(fake_images) - fake_images).std(0)).mean() > 1e-4\n",
    "assert torch.abs((inject_noise(fake_images) - fake_images).std(1)).mean() < 1e-4\n",
    "assert torch.abs((inject_noise(fake_images) - fake_images).std(2)).mean() > 1e-4\n",
    "assert torch.abs((inject_noise(fake_images) - fake_images).std(3)).mean() > 1e-4\n",
    "\n",
    "per_channel_change = (inject_noise(fake_images) - fake_images).mean(1).std()\n",
    "assert per_channel_change > 0.9 and per_channel_change < 1.1\n",
    "\n",
    "inject_noise.weight = nn.Parameter(torch.zeros_like(inject_noise.weight))\n",
    "assert torch.abs((inject_noise(fake_images) - fake_images)).mean() < 1e-4\n",
    "assert len(inject_noise.weight.shape) == 4\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "        self.style_scale_transform = nn.Linear(w_dim, channels)\n",
    "        self.style_shift_transform = nn.Linear(w_dim, channels)\n",
    "\n",
    "    def forward(self, image, w):\n",
    "        normalized_image = self.instance_norm(image)\n",
    "        style_scale = self.style_scale_transform(w)[:, :, None, None]\n",
    "        style_shift = self.style_shift_transform(w)[:, :, None, None]\n",
    "        \n",
    "        transformed_image = style_scale * normalized_image + style_shift\n",
    "\n",
    "        return transformed_image\n",
    "\n",
    "    def get_style_scale_transform(self):\n",
    "        return self.style_scale_transform\n",
    " \n",
    "    def get_style_shift_transform(self):\n",
    "        return self.style_shift_transform\n",
    "\n",
    "    def get_self(self):\n",
    "        return self \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_channels = 50\n",
    "image_channels = 20\n",
    "image_size = 30\n",
    "n_test = 10\n",
    "adain = AdaIN(image_channels, w_channels)\n",
    "test_w = torch.randn(n_test, w_channels)\n",
    "assert adain.style_scale_transform(test_w).shape == adain.style_shift_transform(test_w).shape\n",
    "assert adain.style_scale_transform(test_w).shape[-1] == image_channels\n",
    "assert tuple(adain(torch.randn(n_test, image_channels, image_size, image_size), test_w).shape) == (n_test, image_channels, image_size, image_size)\n",
    "\n",
    "w_channels = 3\n",
    "image_channels = 2\n",
    "image_size = 3\n",
    "n_test = 1\n",
    "adain = AdaIN(image_channels, w_channels)\n",
    "\n",
    "adain.style_scale_transform.weight.data = torch.ones_like(adain.style_scale_transform.weight.data) / 4\n",
    "adain.style_scale_transform.bias.data = torch.zeros_like(adain.style_scale_transform.bias.data)\n",
    "adain.style_shift_transform.weight.data = torch.ones_like(adain.style_shift_transform.weight.data) / 5\n",
    "adain.style_shift_transform.bias.data = torch.zeros_like(adain.style_shift_transform.bias.data)\n",
    "test_input = torch.ones(n_test, image_channels, image_size, image_size)\n",
    "test_input[:, :, 0] = 0\n",
    "test_w = torch.ones(n_test, w_channels)\n",
    "test_output = adain(test_input, test_w)\n",
    "assert(torch.abs(test_output[0, 0, 0, 0] - 3 / 5 + torch.sqrt(torch.tensor(9 / 8))) < 1e-4)\n",
    "assert(torch.abs(test_output[0, 0, 1, 0] - 3 / 5 - torch.sqrt(torch.tensor(9 / 32))) < 1e-4)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicroStyleGANGeneratorBlock(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, w_dim, kernel_size, starting_size, use_upsample=True):\n",
    "        super().__init__()\n",
    "        self.use_upsample = use_upsample\n",
    "        if self.use_upsample:\n",
    "            self.upsample = nn.Upsample((starting_size), mode='bilinear')\n",
    "        self.conv = nn.Conv2d(in_chan, out_chan, kernel_size, padding=1) # Padding is used to maintain the image size\n",
    "        self.inject_noise = InjectNoise(out_chan)\n",
    "        self.adain = AdaIN(out_chan, w_dim)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        if self.use_upsample:\n",
    "            x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.inject_noise(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.adain(x, w)\n",
    "        return x\n",
    "    def get_self(self):\n",
    "        return self;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stylegan_block = MicroStyleGANGeneratorBlock(in_chan=128, out_chan=64, w_dim=256, kernel_size=3, starting_size=8)\n",
    "test_x = torch.ones(1, 128, 4, 4)\n",
    "test_x[:, :, 1:3, 1:3] = 0\n",
    "test_w = torch.ones(1, 256)\n",
    "test_x = test_stylegan_block.upsample(test_x)\n",
    "assert tuple(test_x.shape) == (1, 128, 8, 8)\n",
    "assert torch.abs(test_x.mean() - 0.75) < 1e-4\n",
    "test_x = test_stylegan_block.conv(test_x)\n",
    "assert tuple(test_x.shape) == (1, 64, 8, 8)\n",
    "test_x = test_stylegan_block.inject_noise(test_x)\n",
    "test_x = test_stylegan_block.activation(test_x)\n",
    "assert test_x.min() < 0\n",
    "assert -test_x.min() / test_x.max() < 0.4\n",
    "test_x = test_stylegan_block.adain(test_x, test_w) \n",
    "foo = test_stylegan_block(torch.ones(10, 128, 4, 4), torch.ones(10, 256))\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicroStyleGANGenerator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 z_dim, \n",
    "                 map_hidden_dim,\n",
    "                 w_dim,\n",
    "                 in_chan,\n",
    "                 out_chan, \n",
    "                 kernel_size, \n",
    "                 hidden_chan):\n",
    "        super().__init__()\n",
    "        self.map = MappingLayers(z_dim, map_hidden_dim, w_dim)\n",
    "        self.starting_constant = nn.Parameter(torch.randn(1, in_chan, 4, 4))\n",
    "        self.block0 = MicroStyleGANGeneratorBlock(in_chan, hidden_chan, w_dim, kernel_size, 4, use_upsample=False)\n",
    "        self.block1 = MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 8)\n",
    "        self.block2 = MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 16)\n",
    "        self.block1_to_image = nn.Conv2d(hidden_chan, out_chan, kernel_size=1)\n",
    "        self.block2_to_image = nn.Conv2d(hidden_chan, out_chan, kernel_size=1)\n",
    "        self.alpha = 0.2\n",
    "\n",
    "    def upsample_to_match_size(self, smaller_image, bigger_image):\n",
    "        return F.interpolate(smaller_image, size=bigger_image.shape[-2:], mode='bilinear')\n",
    "\n",
    "    def forward(self, noise, return_intermediate=False):\n",
    "        x = self.starting_constant\n",
    "        w = self.map(noise)\n",
    "        x = self.block0(x, w)\n",
    "        x_small = self.block1(x, w) \n",
    "        x_small_image = self.block1_to_image(x_small)\n",
    "        x_big = self.block2(x_small, w) \n",
    "        x_big_image = self.block2_to_image(x_big)\n",
    "        x_small_upsample = self.upsample_to_match_size(x_small_image, x_big_image)\n",
    "        \n",
    "        interpolation = self.alpha * (x_big_image) + (1-self.alpha) * (x_small_upsample)\n",
    "        \n",
    "        if return_intermediate:\n",
    "            return interpolation, x_small_upsample, x_big_image\n",
    "        return interpolation\n",
    "\n",
    "    def get_self(self):\n",
    "        return self;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 128\n",
    "out_chan = 3\n",
    "truncation = 0.7\n",
    "\n",
    "mu_stylegan = MicroStyleGANGenerator(\n",
    "    z_dim=z_dim, \n",
    "    map_hidden_dim=1024,\n",
    "    w_dim=496,\n",
    "    in_chan=512,\n",
    "    out_chan=out_chan, \n",
    "    kernel_size=3, \n",
    "    hidden_chan=256\n",
    ")\n",
    "\n",
    "test_samples = 10\n",
    "test_result = mu_stylegan(get_truncated_noise(test_samples, z_dim, truncation))\n",
    "\n",
    "assert tuple(test_result.shape) == (test_samples, out_chan, 16, 16)\n",
    "\n",
    "mu_stylegan.alpha = 1.\n",
    "test_result, _, test_big =  mu_stylegan(\n",
    "    get_truncated_noise(test_samples, z_dim, truncation), \n",
    "    return_intermediate=True)\n",
    "assert torch.abs(test_result - test_big).mean() < 0.001\n",
    "mu_stylegan.alpha = 0.\n",
    "test_result, test_small, _ =  mu_stylegan(\n",
    "    get_truncated_noise(test_samples, z_dim, truncation), \n",
    "    return_intermediate=True)\n",
    "assert torch.abs(test_result - test_small).mean() < 0.001\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "\n",
    "viz_samples = 10\n",
    "\n",
    "viz_noise = get_truncated_noise(viz_samples, z_dim, truncation) * 10\n",
    "\n",
    "mu_stylegan.eval()\n",
    "images = []\n",
    "for alpha in np.linspace(0, 1, num=5):\n",
    "    mu_stylegan.alpha = alpha\n",
    "    viz_result, _, _ =  mu_stylegan(\n",
    "        viz_noise, \n",
    "        return_intermediate=True)\n",
    "    images += [tensor for tensor in viz_result]\n",
    "show_tensor_images(torch.stack(images), nrow=viz_samples, num_images=len(images))\n",
    "mu_stylegan = mu_stylegan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
